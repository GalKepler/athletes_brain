{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "109bd8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-30 11:13:15.295\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mathletes_brain.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: /home/galkepler/Projects/athletes_brain\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "\n",
    "from athletes_brain.fig2.config import (\n",
    "    OUTPUT_DIR,\n",
    "    GROUP_NAMES,\n",
    "    REGION_COL,\n",
    "    METRICS,\n",
    "    MODEL_NAME,\n",
    "    DEMOGRAPHIC_COLS,\n",
    ")\n",
    "from athletes_brain.fig2.data_loader import (\n",
    "    load_parcels,\n",
    "    load_and_preprocess_metric_data,\n",
    "    find_common_sessions,\n",
    "    filter_by_common_sessions,\n",
    ")\n",
    "from athletes_brain.fig2.preprocessing import long_to_wide\n",
    "from athletes_brain.fig2.model_training import (\n",
    "    train_base_models,\n",
    "    train_stacked_base_models,\n",
    "    train_final_stacked_model,\n",
    ")\n",
    "from athletes_brain.fig2.utils import save_results, save_predictions, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37763e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Global visualisation configuration ──────────────────────────────────────\n",
    "\n",
    "# 1.  General Matplotlib defaults\n",
    "# ── Global visualisation configuration ──────────────────────────────────────\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "mpl.rcParams.update(\n",
    "    {\n",
    "        # ── Canvas size & resolution ───────────────────────────────────────────\n",
    "        # Default figure size: 12×8 inches  →  4800×3200 px when exported at 400 dpi\n",
    "        \"figure.figsize\": (12, 8),\n",
    "        \"figure.dpi\": 200,  # crisp in-notebook / retina preview\n",
    "        \"savefig.dpi\": 400,  # print-quality PNG/PDF\n",
    "        # ── Fonts ──────────────────────────────────────────────────────────────\n",
    "        \"font.family\": \"sans-serif\",\n",
    "        \"font.sans-serif\": [\"Roboto\", \"DejaVu Sans\", \"Arial\"],\n",
    "        \"axes.titlesize\": 24,\n",
    "        # \"axes.titleweight\": \"bold\",\n",
    "        \"axes.labelsize\": 24,\n",
    "        \"xtick.labelsize\": 14,\n",
    "        \"ytick.labelsize\": 14,\n",
    "        \"legend.fontsize\": 20,\n",
    "        # ── Axis & spine aesthetics ────────────────────────────────────────────\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.spines.left\": True,\n",
    "        \"axes.spines.bottom\": True,\n",
    "        \"axes.linewidth\": 1,\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.color\": \"#E6E6E6\",\n",
    "        \"grid.linewidth\": 0.4,\n",
    "        \"grid.alpha\": 0.8,\n",
    "        # ── Colour cycle (colour-blind-safe) ───────────────────────────────────\n",
    "        \"axes.prop_cycle\": mpl.cycler(color=sns.color_palette(\"Set2\")),\n",
    "        # ── Figure background ─────────────────────────────────────────────────\n",
    "        \"figure.facecolor\": \"white\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Seaborn theme inherits the rcParams above\n",
    "sns.set_theme(context=\"talk\", style=\"whitegrid\", palette=\"Set2\")\n",
    "\n",
    "\n",
    "# 2.  Seaborn theme (inherits Matplotlib rcParams)\n",
    "sns.set_theme(\n",
    "    context=\"talk\",  # slightly larger fonts for presentations / papers\n",
    "    style=\"whitegrid\",  # grid only on y-axis (good for histograms)\n",
    "    palette=\"Set2\",  # matches the rcParams colour cycle\n",
    ")\n",
    "\n",
    "\n",
    "# 3.  Helper function for consistent figure export\n",
    "def savefig_nice(fig, filename, *, tight=True, dpi=300, **savefig_kwargs):\n",
    "    \"\"\"Save figure with tight layout and correct DPI.\"\"\"\n",
    "    if tight:\n",
    "        fig.tight_layout()\n",
    "    fig.savefig(filename, dpi=dpi, bbox_inches=\"tight\", transparent=True, **savefig_kwargs)\n",
    "\n",
    "\n",
    "# 4.  Colour constants for this project (optional convenience)\n",
    "COL_RAW = \"#1f77b4\"  # e.g. unweighted sample\n",
    "COL_WEIGHTED = \"#d62728\"  # weighted sample\n",
    "COL_REF = \"0.35\"  # census reference (neutral grey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b634e331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/galkepler/Projects/athletes_brain/athletes_brain/fig2/data_loader.py:44: DtypeWarning: Columns (1,8,9,10,11,12,68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR / \"processed\" / f\"{metric}.csv\", index_col=0).reset_index(\n",
      "/home/galkepler/Projects/athletes_brain/athletes_brain/fig2/data_loader.py:44: DtypeWarning: Columns (1,8,9,10,11,12,68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR / \"processed\" / f\"{metric}.csv\", index_col=0).reset_index(\n",
      "/home/galkepler/Projects/athletes_brain/athletes_brain/fig2/data_loader.py:44: DtypeWarning: Columns (1,8,9,10,11,12,68) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR / \"processed\" / f\"{metric}.csv\", index_col=0).reset_index(\n",
      "/home/galkepler/Projects/athletes_brain/athletes_brain/fig2/data_loader.py:44: DtypeWarning: Columns (16,23,24,25,26,27,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR / \"processed\" / f\"{metric}.csv\", index_col=0).reset_index(\n",
      "/home/galkepler/Projects/athletes_brain/athletes_brain/fig2/data_loader.py:44: DtypeWarning: Columns (16,23,24,25,26,27,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR / \"processed\" / f\"{metric}.csv\", index_col=0).reset_index(\n",
      "/home/galkepler/Projects/athletes_brain/athletes_brain/fig2/data_loader.py:44: DtypeWarning: Columns (16,23,24,25,26,27,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR / \"processed\" / f\"{metric}.csv\", index_col=0).reset_index(\n",
      "/home/galkepler/Projects/athletes_brain/athletes_brain/fig2/data_loader.py:44: DtypeWarning: Columns (16,23,24,25,26,27,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_DIR / \"processed\" / f\"{metric}.csv\", index_col=0).reset_index(\n"
     ]
    }
   ],
   "source": [
    "ATLAS = \"schaefer2018tian2020_400_7\"\n",
    "region_col = \"index\"\n",
    "# Load important files\n",
    "DATA_DIR = Path(\"/home/galkepler/Projects/athletes_brain/data\")\n",
    "\n",
    "# Output directory for figures\n",
    "OUTPUT_DIR = Path(\"/home/galkepler/Projects/athletes_brain/figures/fig2\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "parcels = load_parcels()\n",
    "raw_metric_data = load_and_preprocess_metric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54eeb890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert to Wide Format and Find Common Sessions\n",
    "data_wide = {}\n",
    "for metric, df in raw_metric_data.items():\n",
    "    data_wide[metric] = long_to_wide(\n",
    "        df, columns_to_pivot=REGION_COL, demographic_cols=DEMOGRAPHIC_COLS\n",
    "    )\n",
    "common_sessions = find_common_sessions(data_wide)\n",
    "data_wide = filter_by_common_sessions(data_wide, common_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a219af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Union\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline  # Make sure this is imported for PolynomialFeatures usage\n",
    "\n",
    "\n",
    "def cross_val_predict_with_bias_correction(\n",
    "    model: BaseEstimator,\n",
    "    X: Union[np.ndarray, pd.DataFrame],\n",
    "    y_chronological: np.ndarray,  # y in paper's notation\n",
    "    cv: BaseCrossValidator,\n",
    "    *,\n",
    "    post_hoc_degree: int = 1,  # 0 = no correction, 1 = linear de Lange/Beheshti, >1 = polynomial residual correction\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Cross-validated predictions with optional post-hoc bias-correction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model            : any scikit-learn regressor or pipeline\n",
    "    X                : shape (n_samples, n_features)\n",
    "    y_chronological  : shape (n_samples,) – chronological age (y in paper's notation)\n",
    "    w                : sample weights, shape (n_samples,)\n",
    "    cv               : cross-validator providing (train_idx, test_idx)\n",
    "    use_weights      : if False, sample weights are ignored\n",
    "    post_hoc_degree  : int\n",
    "                       0 = no bias correction applied.\n",
    "                       1 = linear de Lange et al. / Beheshti-style correction (fit x=a*y+b, correct (x-b)/a).\n",
    "                       >1 = polynomial residual correction (fit residuals as poly of predicted_age, add to prediction).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred_corr      : bias-corrected out-of-fold predictions\n",
    "    original_residuals : uncorrected residuals (chronological_age - original_predicted_age)\n",
    "    corrected_residuals : corrected residuals (chronological_age - corrected_predicted_age)\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure array-like indexing works\n",
    "    X_arr = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y_chronological = np.asarray(y_chronological)\n",
    "\n",
    "    y_oof_corrected = np.full_like(y_chronological, np.nan, dtype=float)\n",
    "    original_residuals = np.full_like(y_chronological, np.nan, dtype=float)\n",
    "    corrected_residuals = np.full_like(y_chronological, np.nan, dtype=float)\n",
    "\n",
    "    for all_train_idx, test_idx in cv.split(X_arr, y_chronological):\n",
    "        # Nested split for training base model and fitting bias correction model\n",
    "        train_idx, val_idx = train_test_split(all_train_idx, test_size=0.2, random_state=42)\n",
    "\n",
    "        X_tr, X_te, X_val = X_arr[train_idx], X_arr[test_idx], X_arr[val_idx]\n",
    "        y_tr, y_te, y_val = (\n",
    "            y_chronological[train_idx],\n",
    "            y_chronological[test_idx],\n",
    "            y_chronological[val_idx],\n",
    "        )\n",
    "        X_tr = pd.DataFrame(X_tr, columns=X.columns) if isinstance(X, pd.DataFrame) else X_tr\n",
    "        X_te = pd.DataFrame(X_te, columns=X.columns) if isinstance(X, pd.DataFrame) else X_te\n",
    "        X_val = pd.DataFrame(X_val, columns=X.columns) if isinstance(X, pd.DataFrame) else X_val\n",
    "        # ---------------- Fit base model -----------------------------\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        # ---------------- Predict on test and validation folds -----------------------\n",
    "        y_pred_te_original = model.predict(X_te)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "\n",
    "        # ---------------- Post-hoc bias correction (conditional on post_hoc_degree) ---------------\n",
    "        if post_hoc_degree > 0:\n",
    "            if post_hoc_degree == 1:\n",
    "                # --- Linear de Lange et al. / Beheshti-style correction ---\n",
    "                # Fit a linear model: x = a*y + b\n",
    "                # Where x is predicted_age (y_pred_val) and y is chronological_age (y_val)\n",
    "                bias_corrector = LinearRegression()\n",
    "                # fit_kwargs_bias = {\"sample_weight\": w_val} if use_weights else {}\n",
    "                fit_kwargs_bias = {}\n",
    "                # Fit: y_pred_val (dependent) on y_val (independent)\n",
    "                bias_corrector.fit(y_val.reshape(-1, 1), y_pred_val, **fit_kwargs_bias)\n",
    "\n",
    "                a_coeff = bias_corrector.coef_[0]  # This is 'a' from x = a*y + b\n",
    "                b_intercept = bias_corrector.intercept_  # This is 'b' from x = a*y + b\n",
    "\n",
    "                # Apply correction: x_corrected = (x - b) / a\n",
    "                if a_coeff != 0:  # Avoid division by zero\n",
    "                    y_pred_te_corrected = (y_pred_te_original - b_intercept) / a_coeff\n",
    "                else:\n",
    "                    y_pred_te_corrected = (\n",
    "                        y_pred_te_original  # No effective correction if no linear relationship\n",
    "                    )\n",
    "            else:  # post_hoc_degree > 1\n",
    "                # --- Polynomial Residual Correction (your original approach) ---\n",
    "                # Fit residuals (y_val - y_pred_val) as a polynomial function of predicted_age (y_pred_val)\n",
    "                poly = PolynomialFeatures(degree=post_hoc_degree)\n",
    "                lin = LinearRegression()\n",
    "\n",
    "                # Create a pipeline for the residual model: poly features of predicted age -> linear regression\n",
    "                resid_model_pipeline = Pipeline([(\"poly\", poly), (\"lin\", lin)])\n",
    "\n",
    "                # Fit the residual model on validation data\n",
    "                # Independent variable: y_pred_val (predicted age on validation set)\n",
    "                # Dependent variable: y_val - y_pred_val (residuals on validation set)\n",
    "                # fit_kwargs_resid_model = {\"lin__sample_weight\": w_val} if use_weights else {}\n",
    "                fit_kwargs_resid_model = {}\n",
    "                resid_model_pipeline.fit(\n",
    "                    y_pred_val.reshape(-1, 1), y_val - y_pred_val, **fit_kwargs_resid_model\n",
    "                )\n",
    "\n",
    "                # Predict the bias component for the test set using its original predictions\n",
    "                bias_component_te = resid_model_pipeline.predict(y_pred_te_original.reshape(-1, 1))\n",
    "\n",
    "                # Add the predicted bias component to the original predictions\n",
    "                y_pred_te_corrected = y_pred_te_original + bias_component_te\n",
    "        else:  # post_hoc_degree == 0, no correction\n",
    "            y_pred_te_corrected = y_pred_te_original\n",
    "\n",
    "        # Store results\n",
    "        y_oof_corrected[test_idx] = y_pred_te_corrected\n",
    "        original_residuals[test_idx] = y_te - y_pred_te_original\n",
    "        corrected_residuals[test_idx] = y_te - y_pred_te_corrected\n",
    "\n",
    "    return y_oof_corrected, original_residuals, corrected_residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcb1d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial features\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Parcel-wise base-learner loop\n",
    "# ---------------------------------------------------------------------\n",
    "stacked_models = parcels.copy()\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "stacked_estimators = {}\n",
    "predictions[\"base_stacked\"] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d9030bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df_template = pd.DataFrame(\n",
    "    index=list(common_sessions),\n",
    "    columns=[\"subject_code\", \"age_at_scan\", \"sex\", \"group\", \"target\"] + list(data_wide.keys()),\n",
    ")\n",
    "\n",
    "# Populate the template with demographic/target info from the first metric\n",
    "first_metric_df = data_wide[list(data_wide.keys())[0]].set_index(\"session_id\")\n",
    "# drop duplicate indexes to avoid issues with multiple sessions\n",
    "first_metric_df = first_metric_df[~first_metric_df.index.duplicated(keep=\"first\")]\n",
    "tmp_df_template[[\"subject_code\", \"age_at_scan\", \"sex\", \"group\", \"target\"]] = first_metric_df.loc[\n",
    "    list(common_sessions), [\"subject_code\", \"age_at_scan\", \"sex\", \"group\", \"target\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c201fa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training stacked base model for Parcel 1 ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from athletes_brain.fig2.preprocessing import init_preprocessor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from athletes_brain.fig2.config import (\n",
    "    AVAILABLE_MODELS,\n",
    "    AVAILABLE_PARAMS,\n",
    "    N_PERMUTATIONS,\n",
    "    MODEL_NAME,\n",
    ")\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "cv = KFold(\n",
    "    n_splits=5, shuffle=True, random_state=42\n",
    ")  # Ensure groups are preserved in cross-validation\n",
    "\n",
    "\n",
    "subjects = tmp_df_template[\"subject_code\"].astype(str)\n",
    "\n",
    "stacked_models_results = parcels.copy()\n",
    "predictions_base_stacked = {}\n",
    "\n",
    "# Initiate CV for subjects to ensure groups are preserved\n",
    "\n",
    "for i, row in parcels.iterrows():\n",
    "    roi = row[REGION_COL]\n",
    "    vals_data = {}\n",
    "    for metric, df in data_wide.items():\n",
    "        m_v = df.set_index(\"session_id\")\n",
    "        m_v = m_v[~m_v.index.duplicated(keep=\"first\")]\n",
    "        vals_data[metric] = m_v[roi].loc[list(common_sessions)]\n",
    "\n",
    "    X_roi = pd.DataFrame(vals_data)\n",
    "    # X_roi[\"age_at_scan\"] = tmp_df_template[\"age_at_scan\"]\n",
    "    X_roi[\"sex\"] = tmp_df_template[\"sex\"]\n",
    "    X_roi = X_roi.loc[tmp_df_template.index]\n",
    "\n",
    "    y = tmp_df_template[\"age_at_scan\"]\n",
    "\n",
    "    X_roi.columns = X_roi.columns.astype(str)\n",
    "\n",
    "    preprocessor = init_preprocessor(X_roi)\n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "            # (\"classifier\", AVAILABLE_MODELS[MODEL_NAME]),\n",
    "            (\"estimator\", RidgeCV(alphas=np.logspace(-3, 3, 7))),\n",
    "        ]\n",
    "    )\n",
    "    print(f\"\\n--- Training stacked base model for Parcel {roi} ---\")\n",
    "    # Fit the base model and get predictions\n",
    "    y_oof_corrected, original_residuals, corrected_residuals = (\n",
    "        cross_val_predict_with_bias_correction(\n",
    "            model=pipe,\n",
    "            X=X_roi,\n",
    "            y_chronological=y,\n",
    "            cv=cv,\n",
    "            post_hoc_degree=1,  # Linear de Lange/Beheshti-style correction\n",
    "        )\n",
    "    )\n",
    "    predictions_base_stacked[roi] = y_oof_corrected\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aaddc231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1169, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_roi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f424ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (base model): 22.66\n",
      "R² (base model): -12.18\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Calculate metrics for the base model\n",
    "mae_base = mean_absolute_error(y, predictions_base_stacked[roi])\n",
    "r2_base = r2_score(y, predictions_base_stacked[roi])\n",
    "print(f\"MAE (base model): {mae_base:.2f}\")\n",
    "print(f\"R² (base model): {r2_base:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2cb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in parcels.iterrows():  # i == parcel index (0..453)\n",
    "    # ------------- build design matrix for parcel i -----------------\n",
    "    # X_roi : (n_subjects , 5 metrics)\n",
    "    # X_cov = cov[cov_names[\"gm_vol\"]].to_numpy()\n",
    "    X_roi = np.hstack([X_dict[m][:, [i]] for m in metrics])\n",
    "    # X_roi = np.hstack([X_roi, X_cov])  # add covariates\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            # (\"poly\", PolynomialFeatures(degree=2)),\n",
    "            (\"estimator\", RidgeCV(alphas=alphas)),\n",
    "            # (\"estimator\", RandomForestRegressor())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---------------- fit model & predict --------------------------\n",
    "    y_pred, original_residuals, corrected_residuals = cross_val_predict_with_bias_correction(\n",
    "        model=pipe,\n",
    "        X=X_roi,\n",
    "        y_chronological=y,\n",
    "        w=w,\n",
    "        cv=outer_cv,\n",
    "        use_weights=use_weights,\n",
    "        post_hoc_degree=0,\n",
    "        # residual_orthog_degree=post_hoc_degree,\n",
    "    )\n",
    "\n",
    "    # ---------------- store predictions & metrics -------------------\n",
    "    pred_df = cov.copy()\n",
    "    pred_df[\"True\"] = y\n",
    "    pred_df[\"Predicted\"] = y_pred\n",
    "    pred_df[\"raw_residuals\"] = original_residuals\n",
    "    pred_df[\"corrected_residuals\"] = corrected_residuals\n",
    "    predictions[\"base_stacked\"][i] = pred_df\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    rmse = root_mean_squared_error(y, y_pred)\n",
    "    r2_weighted = r2_score(y, y_pred, sample_weight=w)\n",
    "    mae_weighted = mean_absolute_error(y, y_pred, sample_weight=w)\n",
    "    rmse_weighted = root_mean_squared_error(y, y_pred, sample_weight=w)\n",
    "\n",
    "    stacked_models.loc[\n",
    "        i, [\"R2\", \"MAE\", \"RMSE\", \"R2_weighted\", \"MAE_weighted\", \"RMSE_weighted\"]\n",
    "    ] = [r2, mae, rmse, r2_weighted, mae_weighted, rmse_weighted]\n",
    "    stacked_estimators[i] = pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7546e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_base_stacked, stacked_models_results, common_data_template = train_stacked_base_models(\n",
    "    common_sessions, data_wide, parcels, group_name, FORCE_STACKING\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
